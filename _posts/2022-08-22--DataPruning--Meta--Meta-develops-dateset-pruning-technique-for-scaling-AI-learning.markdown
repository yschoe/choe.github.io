---
layout: post
title:  "[DataPruning][Meta] Meta develops dateset pruning technique for scaling AI learning"
date:   August 22, 2022
categ:  none
---



Meta develops dataset pruning technique for scaling AI training. 



The main idea is to use self-supervised learning to learn a high-quality pruning metric in a computationally efficient manner. With this, samples can be pruned to some degree without significant loss of performance. 



[https://www.infoq.com/news/2022/08/meta-data-pruning-scale](https://www.infoq.com/news/2022/08/meta-data-pruning-scale)



Full paper can be found here: [https://arxiv.org/pdf/2206.14486.pdf](https://arxiv.org/pdf/2206.14486.pdf)



[Choe] Reminds me of our earlier empirical work on sample-wise learnability. This is pretty computationally intensive though.



Seung-Geon Lee, Jaedeok Kim, Hyun-Joo Jung, and Yoonsuck Choe. Comparing sample-wise learnability across deep neural network models. In Proceedings of the AAAI Conference on Artificial Intelligence (Student Abstract), volume 33, pages 9961-9962, 2019.

[https://ojs.aaai.org/index.php/AAAI/article/view/5117/4990](https://ojs.aaai.org/index.php/AAAI/article/view/5117/4990)

 

